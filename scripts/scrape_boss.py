import requests
import json
import re
import os
import pytz
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime
# import locale
# locale.setlocale(locale.LC_CTYPE, 'chinese')
# ==================== å¸¸æ•¸èˆ‡è¨­å®šå€å¡Š START ====================

# ç›®æ¨™ç¶²ç«™èˆ‡å‚™ä»½ä¾†æº
RAID_URL = "https://leekduck.com/boss/"
BASE_URL = "https://leekduck.com/"
FALLBACK_URL = "https://raw.githubusercontent.com/bigfoott/ScrapedDuck/data/raids.min.json"

# æ¨¡æ“¬ç€è¦½å™¨æ¨™é ­
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# è¼¸å‡ºè³‡æ–™å¤¾è·¯å¾‘
OUTPUT_DIR = "./data"
OUTPUT_FILENAME_PRETTY = os.path.join(OUTPUT_DIR, "raids.json")
OUTPUT_FILENAME_MIN = os.path.join(OUTPUT_DIR, "raids.min.json")


# ==================== ç¿»è­¯è³‡æ–™å€å¡Š START ====================
# (æ²¿ç”¨æ‚¨ç¯„æœ¬ä¸­çš„ç¿»è­¯é‚è¼¯)

# å®šç¾©ç‰¹æ®Šå½¢æ…‹çš„ç¿»è­¯å­—å…¸
FORM_TRANSLATIONS = {
    'Alolan': 'é˜¿ç¾…æ‹‰', 'Galarian': 'ä¼½å‹’çˆ¾', 'Hisuian': 'æ´—ç¿ ', 'Origin': 'èµ·æº',
    'Altered': 'åˆ¥ç¨®', 'Attack': 'æ”»æ“Š', 'Defense': 'é˜²ç¦¦', 'Speed': 'é€Ÿåº¦',
    'Sunny': 'å¤ªé™½', 'Rainy': 'é›¨æ°´', 'Snowy': 'é›ªé›²', 'Overcast': 'é™°å¤©',
    'Incarnate': 'åŒ–èº«', 'Therian': 'éˆç¸', 'Black': 'é—‡é»‘', 'White': 'ç„°ç™½',
    'Plant': 'è‰æœ¨', 'Sandy': 'ç ‚åœŸ', 'Trash': 'åƒåœ¾', 'Confined': 'æ‡²æˆ’', 'Unbound': 'è§£æ”¾',
    'Aria': 'æ­Œè²', 'Pirouette': 'èˆæ­¥', 'Fan': 'é›»é¢¨æ‰‡', 'Frost': 'çµå†°', 'Heat': 'åŠ ç†±', 'Mow': 'å‰²è‰', 'Wash': 'æ¸…æ´—',
    'Dusk': 'é»ƒæ˜', 'Midday': 'ç™½æ™', 'Midnight': 'é»‘å¤œ', 'School': 'é­šç¾¤', 'Solo': 'å–®ç¨', 'Dawn Wings': 'æ—¥è•å¥ˆå…‹æ´›èŒ²ç‘ª', 'Ultra': 'ç©¶æ¥µ',
    'Hero': 'ç™¾æˆ°å‹‡è€…', 'White Striped': 'ç™½æ¢ç´‹', 'Mega': 'è¶…ç´š', 'Primal': 'åŸå§‹', "Farfetch'd": "å¤§è”¥é¸­"
}

def load_translations():
    """è¼‰å…¥å¯¶å¯å¤¢åç¨±ç¿»è­¯æª”"""
    translation_file = os.path.join(OUTPUT_DIR, "pokemon_translation_map.json")
    try:
        with open(translation_file, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âŒ è­¦å‘Šï¼šæ‰¾ä¸åˆ°ç¿»è­¯æª” {translation_file}ï¼Œéƒ¨åˆ†åç¨±å¯èƒ½ç„¡æ³•ç¿»è­¯ã€‚")
        return {} # å›å‚³ç©ºå­—å…¸ä»¥é¿å…ç¨‹å¼ä¸­æ–·
    except json.JSONDecodeError:
        print(f"âŒ éŒ¯èª¤ï¼š{translation_file} æ ¼å¼æœ‰èª¤ã€‚")
        return None

def translate_name(eng_name, name_map, form_map):
    """å°‡è‹±æ–‡åç¨±ç¿»è­¯æˆä¸­æ–‡ï¼Œèƒ½è™•ç†åœ°å€å½¢æ…‹ã€è¶…ç´šé€²åŒ–å’Œç‰¹æ®Šåç¨±"""
    if not eng_name:
        return ""

    # è™•ç†è¶…ç´šé€²åŒ–ï¼Œä¾‹å¦‚ "Mega Charizard X"
    if eng_name.startswith("Mega ") or eng_name.startswith("Primal "):
        parts = eng_name.split(" ", 1)
        form_type = parts[0] # "Mega" æˆ– "Primal"
        base_name = parts[1]
        
        translated_base = name_map.get(base_name, base_name)
        translated_form = form_map.get(form_type, form_type)
        return f"{translated_form} {translated_base}"

    # è™•ç†æ‹¬è™Ÿå…§çš„ç‰¹æ®Šå½¢æ…‹, e.g., "Basculin (White Striped)"
    match = re.match(r'(.+?)\s*\((.+?)\)', eng_name)
    if match:
        base_name, form_name = match.groups()
        base_name = base_name.strip()
        form_name = form_name.strip()
        if base_name in name_map and form_name in form_map:
            return f"{name_map[base_name]} ({form_map[form_name]})"

    # è™•ç†åœ°å€å½¢æ…‹, e.g., "Alolan Meowth"
    parts = eng_name.split()
    if len(parts) > 1 and parts[0] in form_map:
        form_name, base_name = parts[0], " ".join(parts[1:])
        if base_name in name_map:
            return f"{form_map[form_name]} {name_map[base_name]}"

    # è™•ç†ä¸€èˆ¬åç¨±
    if eng_name in name_map:
        return name_map[eng_name]

    print(f"  > ç¿»è­¯è­¦å‘Šï¼šåœ¨å­—å…¸ä¸­æ‰¾ä¸åˆ° '{eng_name}'ï¼Œå°‡ä½¿ç”¨åŸæ–‡ã€‚")
    return eng_name

# ==================== ç¿»è­¯è³‡æ–™å€å¡Š END ======================


def scrape_raid_data():
    """ä¸»å‡½å¼ï¼Œç”¨æ–¼æŠ“å–ã€è§£æå’Œå„²å­˜åœ˜é«”æˆ°é ­ç›®è³‡æ–™"""
    
    # å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾ (å¦‚æœä¸å­˜åœ¨)
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # è¼‰å…¥ç¿»è­¯è³‡æ–™
    name_translation_map = load_translations()
    if name_translation_map is None: # åƒ…åœ¨JSONæ ¼å¼éŒ¯èª¤æ™‚åœæ­¢
        return
        
    bosses = []
    
    try:
        # --- ä¸»è¦æŠ“å–é‚è¼¯ ---
        print(f"ğŸš€ æ­£åœ¨å¾ {RAID_URL} æŠ“å–åœ˜é«”æˆ°è³‡æ–™...")
        response = requests.get(RAID_URL, headers=HEADERS, timeout=15)
        response.raise_for_status() # å¦‚æœè«‹æ±‚å¤±æ•— (å¦‚ 404, 500)ï¼Œæœƒæ‹‹å‡ºä¾‹å¤–
        print("âœ… æˆåŠŸå–å¾—ç¶²é å…§å®¹ï¼æ­£åœ¨è§£æ...")

        soup = BeautifulSoup(response.text, "lxml")
        
        # æ‰¾åˆ°æ‰€æœ‰ tier çš„å€å¡Š (åŒ…å«ä¸€èˆ¬å’Œæš—å½±é ­ç›®)
        all_tiers = soup.select(".raid-bosses .tier, .shadow-raid-bosses .tier")
        if not all_tiers:
            raise ValueError("åœ¨é é¢ä¸­æ‰¾ä¸åˆ° '.tier' å€å¡Šã€‚")

        for tier_div in all_tiers:
            # å–å¾— tier åç¨±
            header = tier_div.find("h2", class_="header")
            if not header:
                continue
            current_tier = header.get_text(strip=True)
            print(f"\n--- æ­£åœ¨è™•ç†åˆ†é¡ï¼š{current_tier} ---")

            # æ‰¾åˆ°é€™å€‹ tier å…§æ‰€æœ‰çš„é ­ç›®å¡ç‰‡
            for card in tier_div.find_all("div", class_="card"):
                # æŠ“å–è‹±æ–‡åç¨±ä¸¦ç¿»è­¯
                name_tag = card.find("p", class_="name")
                if not name_tag:
                    continue
                eng_name = name_tag.get_text(strip=True)
                translated_boss_name = translate_name(eng_name, name_translation_map, FORM_TRANSLATIONS)
                print(f"  - æ‰¾åˆ°é ­ç›®: {translated_boss_name} ({eng_name})")

                # è§£æCPç¯„åœ
                cp_range_tag = card.find("div", class_="cp-range")
                cp_text = cp_range_tag.get_text(strip=True).replace("CP", "").strip()
                cp_min, cp_max = [int(p.strip()) for p in cp_text.split(' - ')]

                # è§£æå¤©æ°£åŠ æˆå¾Œçš„CPç¯„åœèˆ‡å¤©æ°£é¡å‹
                boost_section = card.find("div", class_="weather-boosted")
                boosted_cp_tag = boost_section.find("span", class_="boosted-cp")
                boosted_cp_text = boosted_cp_tag.get_text(strip=True).replace("CP", "").strip()
                boosted_cp_min, boosted_cp_max = [int(p.strip()) for p in boosted_cp_text.split(' - ')]
                
                boosted_weathers = []
                for img in boost_section.select(".boss-weather img"):
                    boosted_weathers.append({
                        "name": img['title'].lower(),
                        "image": urljoin(BASE_URL, img['src'])
                    })

                # è§£æå¯¶å¯å¤¢å±¬æ€§
                types = []
                for img in card.select(".boss-type img"):
                    types.append({
                        "name": img['title'].lower(),
                        "image": urljoin(BASE_URL, img['src'])
                    })

                # æŠ“å–å¯¶å¯å¤¢åœ–ç‰‡
                boss_img_tag = card.select_one(".boss-img img")
                image_url = urljoin(BASE_URL, boss_img_tag['src']) if boss_img_tag else ""

                # çµ„åˆæœ€çµ‚çš„ JSON ç‰©ä»¶
                boss_data = {
                    "name": translated_boss_name,
                    "englishName": eng_name,
                    "tier": current_tier,
                    "canBeShiny": card.find("svg", class_="shiny-icon") is not None,
                    "types": types,
                    "combatPower": {
                        "normal": {"min": cp_min, "max": cp_max},
                        "boosted": {"min": boosted_cp_min, "max": boosted_cp_max}
                    },
                    "boostedWeather": boosted_weathers,
                    "image": image_url
                }
                bosses.append(boss_data)

    except (requests.exceptions.RequestException, ValueError) as e:
        # --- å‚™ä»½æŠ“å–é‚è¼¯ (Fallback) ---
        print(f"\nâŒ ä¸»è¦æŠ“å–å¤±æ•— ({e})ï¼Œå˜—è©¦å¾å‚™ä»½ä¾†æºä¸‹è¼‰...")
        try:
            response = requests.get(FALLBACK_URL, timeout=15)
            response.raise_for_status()
            fallback_data = response.json()
            print("âœ… æˆåŠŸå¾å‚™ä»½ä¾†æºä¸‹è¼‰è³‡æ–™ï¼")
            
            # ç¢ºä¿å¾ fallback_data æ­£ç¢ºæå– bosses åˆ—è¡¨
            boss_list_from_fallback = fallback_data.get('bosses', []) if isinstance(fallback_data, dict) else fallback_data
            
            # å‚™ä»½è³‡æ–™ä¹Ÿéœ€è¦ç¿»è­¯
            for boss in boss_list_from_fallback:
                if 'name' in boss:
                    original_name = boss['name']
                    boss['englishName'] = original_name
                    boss['name'] = translate_name(original_name, name_translation_map, FORM_TRANSLATIONS)
            
            bosses = boss_list_from_fallback

        except requests.exceptions.RequestException as fallback_e:
            print(f"âŒ å‚™ä»½ä¾†æºä¹ŸæŠ“å–å¤±æ•—: {fallback_e}")
            return # é›™é‡å¤±æ•—ï¼Œç›´æ¥çµæŸç¨‹å¼
    
    except Exception as e:
        print(f"âŒ è™•ç†è³‡æ–™æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}")
        return

    # --- å¯«å…¥æª”æ¡ˆ ---
    if bosses:
        print("\nğŸ’¾ æ­£åœ¨å°‡è³‡æ–™å¯«å…¥æª”æ¡ˆ...")
        taipei_tz = pytz.timezone('Asia/Taipei')
        now_taipei = datetime.now(pytz.utc).astimezone(taipei_tz)
        try:
            # å¯«å…¥æ ¼å¼åŒ–çš„ JSON
            output_data = {
                "lastUpdated": now_taipei.strftime('%Yå¹´%mæœˆ%dæ—¥%Hæ™‚').replace('å¹´0', 'å¹´').replace('æœˆ0', 'æœˆ'),
                "bosses": bosses
            }
        
            with open(OUTPUT_FILENAME_PRETTY, "w", encoding="utf-8") as f:
                json.dump(output_data, f, indent=4, ensure_ascii=False)
            
            # å¯«å…¥å£“ç¸®çš„ JSON
            with open(OUTPUT_FILENAME_MIN, "w", encoding="utf-8") as f:
                json.dump(output_data, f, ensure_ascii=False)

            print(f"âœ… æˆåŠŸï¼ è³‡æ–™å·²å„²å­˜è‡³ {OUTPUT_FILENAME_PRETTY} å’Œ {OUTPUT_FILENAME_MIN}")
            print(f"ç¸½å…±æ“·å–åˆ° {len(bosses)} ç­†é ­ç›®è³‡æ–™ã€‚")
        except IOError as e:
            print(f"âŒ å¯«å…¥æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    else:
        print("ğŸ¤·â€â™‚ï¸ æ²’æœ‰æŠ“å–åˆ°ä»»ä½•è³‡æ–™ï¼Œä¸å¯«å…¥æª”æ¡ˆã€‚")


if __name__ == "__main__":
    scrape_raid_data()
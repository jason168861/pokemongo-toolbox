import requests
import json
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# ==================== ç¿»è­¯è³‡æ–™å€å¡Š START ====================

# ã€ä¿®æ”¹é» 1ã€‘æ“´å……ç¿»è­¯å­—å…¸ï¼Œä¸¦æ›´åç‚ºæ›´é€šç”¨çš„ GENERAL_TRANSLATIONS
# (æ­¤å€å¡Šä¿æŒä¸è®Š)
GENERAL_TRANSLATIONS = {
    # å¯¶å¯å¤¢å½¢æ…‹
    'Alolan': 'é˜¿ç¾…æ‹‰', 'Galarian': 'ä¼½å‹’çˆ¾', 'Hisuian': 'æ´—ç¿ ', 'Origin': 'èµ·æº',
    'Altered': 'åˆ¥ç¨®', 'Attack': 'æ”»æ“Š', 'Defense': 'é˜²ç¦¦', 'Speed': 'é€Ÿåº¦',
    'Sunny': 'å¤ªé™½', 'Rainy': 'é›¨æ°´', 'Snowy': 'é›ªé›²', 'Overcast': 'é™°å¤©',
    'Incarnate': 'åŒ–èº«', 'Therian': 'éˆç¸', 'Black': 'é—‡é»‘', 'White': 'ç„°ç™½',
    'Plant': 'è‰æœ¨', 'Sandy': 'ç ‚åœŸ', 'Trash': 'åƒåœ¾', 'Confined': 'æ‡²æˆ’', 'Unbound': 'è§£æ”¾',
    'Aria': 'æ­Œè²', 'Pirouette': 'èˆæ­¥', 'Fan': 'é›»é¢¨æ‰‡', 'Frost': 'çµå†°', 'Heat': 'åŠ ç†±', 'Mow': 'å‰²è‰', 'Wash': 'æ¸…æ´—',
    'Dusk': 'é»ƒæ˜', 'Midday': 'ç™½æ™', 'Midnight': 'é»‘å¤œ', 'School': 'é­šç¾¤', 'Solo': 'å–®ç¨', 'Dawn Wings': 'æ—¥è•å¥ˆå…‹æ´›èŒ²ç‘ª', 'Ultra': 'ç©¶æ¥µ',
    'Hero': 'ç™¾æˆ°å‹‡è€…', 'White Striped': 'ç™½æ¢ç´‹',
    
    # é“å…· & è³‡æº
    'Stardust': 'æ˜Ÿæ˜Ÿæ²™å­',
    'Rare Candy': 'ç¥å¥‡ç³–æœ',
    'Rare Candy XL': 'ç¥å¥‡ç³–æœXL',
    'Golden Razz Berry': 'é‡‘è“æœ',
    'Razz Berry': 'è”“è“æœ',
    'Poffin': 'å¯¶èŠ¬',
    'Silver Pinap Berry': 'éŠ€è‰²å‡°æ¢¨æœ',
    'Pinap Berry': 'å‡°æ¢¨æœ',
    'Ultra Ball': 'é«˜ç´šçƒ',
    'Great Ball': 'è¶…ç´šçƒ',
    'PokÃ© Ball': 'ç²¾éˆçƒ',
    'Sinnoh Stone': 'ç¥å¥§ä¹‹çŸ³',
    'Unova Stone': 'åˆçœ¾ä¹‹çŸ³',
    'Mega Energy': 'è¶…ç´šèƒ½é‡' # ç”¨æ–¼çµ„åˆï¼Œä¾‹å¦‚ "å¦™è›™èŠ± è¶…ç´šèƒ½é‡"
}

def load_json_map(filename):
    """é€šç”¨å‡½å¼ï¼Œç”¨æ–¼è¼‰å…¥ JSON æ ¼å¼çš„ç¿»è­¯æª”"""
    try:
        with open(filename, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"ğŸ“„ æç¤ºï¼šæ‰¾ä¸åˆ°å¯é¸çš„ç¿»è­¯æª” {filename}ã€‚")
        return {}
    except json.JSONDecodeError:
        print(f"âŒ éŒ¯èª¤ï¼š{filename} æ ¼å¼æœ‰èª¤ã€‚")
        return {}

def translate_name(eng_name, name_map, form_map):
    """å°‡å¯¶å¯å¤¢è‹±æ–‡åç¨±ç¿»è­¯æˆä¸­æ–‡"""
    if not eng_name or not name_map:
        return eng_name
    
    form_in_parentheses_match = re.match(r'(.+?)\s*\((.+?)\)', eng_name)
    if form_in_parentheses_match:
        base_name, form_name = form_in_parentheses_match.groups()
        base_name, form_name = base_name.strip(), form_name.strip()
        if base_name in name_map and form_name in form_map:
            return f"{name_map[base_name]} ({form_map[form_name]})"

    parts = eng_name.split()
    if len(parts) == 2 and parts[0] == 'Spinda' and parts[1].isdigit():
        return f"æ™ƒæ™ƒæ–‘ {parts[1]} è™Ÿ"

    if len(parts) > 1 and parts[0] in form_map:
        form_name = parts[0]
        base_name = " ".join(parts[1:])
        if base_name in name_map:
            return f"{form_map[form_name]} {name_map[base_name]}"

    if eng_name in name_map:
        return name_map[eng_name]
    
    return eng_name

# ==================================================================
#                    ã€â­ï¸ é€™è£¡æ˜¯ä¿®æ­£çš„æ ¸å¿ƒ â­ï¸ã€‘
# å°‡ä¸‹æ–¹çš„ translate_item_resource å‡½å¼æ›¿æ›æ‰æ‚¨åŸæœ¬çš„å‡½å¼
# ==================================================================
def translate_item_resource(eng_name, general_map, pokemon_map):
    """ç¿»è­¯é“å…·ã€è³‡æºï¼Œä¸¦è™•ç†åƒè¶…ç´šèƒ½é‡å’Œå¸¶æœ‰æ•¸é‡çš„é“å…·ç­‰ç‰¹æ®Šæƒ…æ³ã€‚"""
    # å„ªå…ˆè™•ç†è¶…ç´šèƒ½é‡ï¼Œä¾‹å¦‚ "Venusaur Mega Energy"
    mega_energy_match = re.match(r'(.+?)\s+Mega Energy', eng_name)
    if mega_energy_match:
        pokemon_name = mega_energy_match.group(1).strip()
        translated_pokemon = pokemon_map.get(pokemon_name, pokemon_name)
        translated_mega_energy = general_map.get('Mega Energy', 'Mega Energy')
        return f"{translated_pokemon} {translated_mega_energy}"

    # æŒ‰é•·åº¦é™åºæ’åºå­—å…¸éµï¼Œä»¥å„ªå…ˆåŒ¹é…æ›´é•·çš„åç¨±
    # ä¾‹å¦‚ï¼Œç¢ºä¿ "Golden Razz Berry" æ¯” "Razz Berry" å…ˆè¢«åŒ¹é…
    sorted_item_keys = sorted(general_map.keys(), key=len, reverse=True)

    for item_key in sorted_item_keys:
        # æª¢æŸ¥å‚³å…¥çš„è‹±æ–‡åæ˜¯å¦ä»¥å­—å…¸ä¸­çš„æŸå€‹éµï¼ˆé“å…·åï¼‰é–‹é ­
        if eng_name.startswith(item_key):
            # å¦‚æœæ˜¯ï¼Œå‰‡æ‰¾åˆ°äº†æˆ‘å€‘çš„é“å…·
            # å–å¾—é“å…·åç¨±å¾Œé¢çš„å‰©é¤˜éƒ¨åˆ†ï¼ˆé€šå¸¸æ˜¯æ•¸é‡ï¼Œä¾‹å¦‚ " Ã—1500"ï¼‰
            remaining_part = eng_name[len(item_key):].strip()
            
            # ç¿»è­¯é“å…·åç¨±
            translated_item = general_map[item_key]
            
            # å°‡ç¿»è­¯å¾Œçš„åç¨±èˆ‡å‰©é¤˜çš„æ•¸é‡éƒ¨åˆ†é‡æ–°çµ„åˆ
            return f"{translated_item} {remaining_part}".strip()
            
    # å¦‚æœéæ­·å®Œå­—å…¸éƒ½æ‰¾ä¸åˆ°åŒ¹é…çš„é–‹é ­ï¼Œå‰‡è¿”å›åŸå§‹åç¨±
    return eng_name

# ==================== ç¿»è­¯è³‡æ–™å€å¡Š END ======================

URL = "https://leekduck.com/research/"
BASE_URL = "https://leekduck.com/"
HEADERS = { "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36" }

def scrape_research_data_full():
    pokemon_name_map = load_json_map("./data/pokemon_translation_map.json")
    task_translation_map = load_json_map("./scripts/task_translation_map.json") 
    category_translation_map = load_json_map("./scripts/category_translation_map.json") # <-- æ–°å¢æ­¤è¡Œ

    print("â³ æ­£åœ¨å¾ LeekDuck çš„ç”°é‡èª¿æŸ¥é é¢æŠ“å–è³‡æ–™...")
    response = requests.get(URL, headers=HEADERS)
    response.raise_for_status()
    print("âœ… æˆåŠŸå–å¾—ç¶²é å…§å®¹ï¼")

    soup = BeautifulSoup(response.text, "lxml")
    
    research_data = []
    task_categories = soup.find_all("div", class_="task-category")

    for category in task_categories:
        category_title_eng = category.find("h2").text.strip()
        translated_category = category_translation_map.get(category_title_eng, category_title_eng)

        category_data = {
            "category": translated_category,
            "tasks": []
        }
        print(f"ğŸ” æ­£åœ¨è™•ç†åˆ†é¡ï¼š{translated_category}")

        for task_item in category.find_all("li", class_="task-item"):
            task_text_span = task_item.find("span", class_="task-text")
            if not task_text_span: continue
            
            task_description = task_text_span.text.strip()
            translated_task = task_translation_map.get(task_description, task_description)

            task_data = {
                "description": translated_task,
                "rewards": []
            }
            
            reward_list = task_item.find("ul", class_="reward-list")
            if not reward_list: continue

            for reward_li in reward_list.find_all("li", class_="reward", recursive=False):
                reward_info = {}
                reward_type = reward_li.get("data-reward-type")
                
                reward_info['isFeatured'] = reward_li.get("data-reward-filter") == "less"
                
                if reward_type == "encounter":
                    reward_info['type'] = 'encounter'
                    name = reward_li.find("span", class_="reward-label").find("span").text.strip()
                    reward_info['name'] = translate_name(name, pokemon_name_map, GENERAL_TRANSLATIONS)
                    reward_info['imageUrl'] = urljoin(BASE_URL, reward_li.find("img", class_="reward-image")['src'])
                    reward_info['isShiny'] = reward_li.find("img", class_="shiny-icon") is not None
                    
                    max_cp_tag = reward_li.find("span", class_="max-cp")
                    min_cp_tag = reward_li.find("span", class_="min-cp")
                    reward_info['maxCp'] = max_cp_tag.text.replace('Max CP', '').strip() if max_cp_tag else None
                    reward_info['minCp'] = min_cp_tag.text.replace('Min CP', '').strip() if min_cp_tag else None
                
                elif reward_type in ["item", "resource"]:
                    # å°‡æ‰€æœ‰è³‡è¨Šéƒ½å¡«å…¥ reward_infoï¼Œä½†ä¸åœ¨æ­¤è™• append
                    reward_info['type'] = 'item'
                    reward_info['pokemonIconUrl'] = None

                    reward_bubble = reward_li.find("span", class_="reward-bubble")
                    if reward_bubble:
                        # æŠ“å–ä¸»è¦é“å…·åœ–ç‰‡
                        main_image_tag = reward_bubble.find("img", class_="reward-image", recursive=False)
                        if main_image_tag and main_image_tag.has_attr('src'):
                            reward_info['imageUrl'] = urljoin(BASE_URL, main_image_tag['src'])

                        # å°ˆé–€ç‚ºè¶…ç´šèƒ½é‡æŠ“å–é¡å¤–çš„å¯¶å¯å¤¢åœ–ç¤º
                        resource_info_div = reward_bubble.find("div", class_="resource-info")
                        if resource_info_div:
                            pokemon_icon_tag = resource_info_div.find("img")
                            if pokemon_icon_tag and pokemon_icon_tag.has_attr('src'):
                                reward_info['pokemonIconUrl'] = urljoin(BASE_URL, pokemon_icon_tag['src'])

                        # æŠ“å–é“å…·åç¨±
                        label_span = reward_li.find("span", class_="reward-label")
                        if label_span:
                            eng_name = label_span.text.strip()
                            reward_info['name'] = translate_item_resource(eng_name, GENERAL_TRANSLATIONS, pokemon_name_map)
                        
                        # æŠ“å–æ•¸é‡
                        quantity_tag = reward_bubble.find("div", class_="quantity")
                        if quantity_tag:
                            reward_info['quantity'] = quantity_tag.text.strip()
                
                if reward_info:
                    task_data["rewards"].append(reward_info)
            
            if task_data["rewards"]:
                category_data["tasks"].append(task_data)
        
        if category_data["tasks"]:
            research_data.append(category_data)

    output_filename = "./data/research.json"
    with open(output_filename, "w", encoding="utf-8") as f:
        json.dump(research_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ‰ æˆåŠŸï¼ ç”°é‡èª¿æŸ¥è³‡æ–™å·²å„²å­˜è‡³ {output_filename}")
    
if __name__ == "__main__":
    scrape_research_data_full()